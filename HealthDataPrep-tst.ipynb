{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Health Data\n",
    "### Atheer Al Attar, March 2018\n",
    "These are a collection of tweets from several health news agencies along the time, divided into several files each for an agency. We will try to import the data, visualize it and perform exploratory data analysis. The source of the data is University of California at Irvine repo.\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "1. Data Import and unzipping.\n",
    "2. Reading the directory, concatenating the different files into one\n",
    "3. Reading and aggregating text files.\n",
    "4. Preparing the corpus, data cleaning\n",
    "5. Word Frequencies generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Import and unzipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip\")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, urllib.request, shutil\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip'\n",
    "file_name = 'myzip.zip'\n",
    "zip_ref = zipfile.ZipFile(\"./myzip.zip\", 'r')\n",
    "zip_ref.extractall(\"./\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reading the directory, concatenating the different files into one\n",
    "We will use the OS package to list the files names into a list and then iterate over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Health-Tweets/cbchealth.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "files_list = glob.glob(\"./Health-Tweets/*.txt\")\n",
    "files_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reading the text files\n",
    "I noticed that some of the files have several rows that are mistakenly formatted, I used the code below to skip them. I performed some housekeeping in this step, renamed columns, changed the date type to the correct type. I have also corrected the index to be consistent other than the files individual index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "df=pd.DataFrame()\n",
    "for file in files_list:\n",
    "    try:\n",
    "        #x=df.append(pd.read_csv(file, sep=\"|\", comment=\"#\", na_values=['Nothing'],header=None))\n",
    "        x=pd.read_csv(file, sep=\"|\", comment=\"#\", na_values=['Nothing'],header=None)\n",
    "        x['file_name']=file\n",
    "        df=df.append(x)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preparing the corpus, data cleaning\n",
    "In this stage the text data will be cleaned from punctuations, stop words and word counts column will be added, in addition to naming the rest of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns=['id','date','tweet','source']\n",
    "df.drop(['id'], axis=1)\n",
    "df.date=pd.to_datetime(df.date)\n",
    "df['index']=range(len(df))\n",
    "df.index=df['index']\n",
    "df['source'] = df.source.str[16:]\n",
    "\n",
    "#Generating word counts\n",
    "df['word_count'] = df['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "#Removing Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(['would', 'like'] )\n",
    "df.tweet=df.tweet.apply(str)\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "#Removing Punctuations\n",
    "df['tweet']=df['tweet'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#removing the http links\n",
    "\n",
    "df['tweet']=df['tweet'].str.replace('(\\s)http\\w+','')\n",
    "\n",
    "#check this website for more info on regex https://regex101.com\n",
    "\n",
    "df.drop('id', inplace=True, axis=1)\n",
    "\n",
    "df.date=pd.to_datetime(df.date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Word Frequencies Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_word_count(string,dic):\n",
    "    for word in string.split(\" \"):\n",
    "        if word in dic.keys():\n",
    "            dic[word]=dic[word]+1\n",
    "        else:\n",
    "            dic[word]=1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "dic={}\n",
    "for tweets in df.tweet:\n",
    "    row_word_count(tweets,dic)\n",
    "\n",
    "sorted_d = sorted(dic.items(), key=operator.itemgetter(1), reverse=True)\n",
    "word_counts=pd.DataFrame(sorted_d,columns=['Word','Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts=word_counts.iloc[1:20,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to do next\n",
    "1- Plot highest word by agency\n",
    "2- Plot a timeline for\n",
    "3- Number of tweets per agency per time\n",
    "4- word cloud per agency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
